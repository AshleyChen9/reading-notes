- [相关与非参数检验-回归](#%e7%9b%b8%e5%85%b3%e4%b8%8e%e9%9d%9e%e5%8f%82%e6%95%b0%e6%a3%80%e9%aa%8c-%e5%9b%9e%e5%bd%92)
  - [线性方程与回归](#%e7%ba%bf%e6%80%a7%e6%96%b9%e7%a8%8b%e4%b8%8e%e5%9b%9e%e5%bd%92)
    - [线性方程](#%e7%ba%bf%e6%80%a7%e6%96%b9%e7%a8%8b)
    - [最小二乘](#%e6%9c%80%e5%b0%8f%e4%ba%8c%e4%b9%98)
    - [标准化回归方程](#%e6%a0%87%e5%87%86%e5%8c%96%e5%9b%9e%e5%bd%92%e6%96%b9%e7%a8%8b)
    - [估计的标准误](#%e4%bc%b0%e8%ae%a1%e7%9a%84%e6%a0%87%e5%87%86%e8%af%af)
    - [标准误与相关的关系](#%e6%a0%87%e5%87%86%e8%af%af%e4%b8%8e%e7%9b%b8%e5%85%b3%e7%9a%84%e5%85%b3%e7%b3%bb)
  - [回归方程的显著性检验：回归分析](#%e5%9b%9e%e5%bd%92%e6%96%b9%e7%a8%8b%e7%9a%84%e6%98%be%e8%91%97%e6%80%a7%e6%a3%80%e9%aa%8c%e5%9b%9e%e5%bd%92%e5%88%86%e6%9e%90)
  - [有两个预测变量的多元回归](#%e6%9c%89%e4%b8%a4%e4%b8%aa%e9%a2%84%e6%b5%8b%e5%8f%98%e9%87%8f%e7%9a%84%e5%a4%9a%e5%85%83%e5%9b%9e%e5%bd%92)
    - [两个预测变量的回归方程](#%e4%b8%a4%e4%b8%aa%e9%a2%84%e6%b5%8b%e5%8f%98%e9%87%8f%e7%9a%84%e5%9b%9e%e5%bd%92%e6%96%b9%e7%a8%8b)
    - [回归方差所占的百分比与残差](#%e5%9b%9e%e5%bd%92%e6%96%b9%e5%b7%ae%e6%89%80%e5%8d%a0%e7%9a%84%e7%99%be%e5%88%86%e6%af%94%e4%b8%8e%e6%ae%8b%e5%b7%ae)
    - [根据残差计算$R^2$与$1-R^2$](#%e6%a0%b9%e6%8d%ae%e6%ae%8b%e5%b7%ae%e8%ae%a1%e7%ae%97r2%e4%b8%8e1-r2)
    - [估计的标准误](#%e4%bc%b0%e8%ae%a1%e7%9a%84%e6%a0%87%e5%87%86%e8%af%af-1)
    - [多元回归方程的显著性检验：回归分析](#%e5%a4%9a%e5%85%83%e5%9b%9e%e5%bd%92%e6%96%b9%e7%a8%8b%e7%9a%84%e6%98%be%e8%91%97%e6%80%a7%e6%a3%80%e9%aa%8c%e5%9b%9e%e5%bd%92%e5%88%86%e6%9e%90)
  - [评估每个预测值的贡献](#%e8%af%84%e4%bc%b0%e6%af%8f%e4%b8%aa%e9%a2%84%e6%b5%8b%e5%80%bc%e7%9a%84%e8%b4%a1%e7%8c%ae)

## 相关与非参数检验-回归
### 线性方程与回归
用于找出一组数据的最佳拟合直线的统计技术被称为`回归`，作为结果的直线被称为回归线。

#### 线性方程
通常，在两个变量X与Y之间的线性关系，可以被表示为公式：$Y=bX+a\ (17.1)$，a与b是固定的常数。

#### 最小二乘
为了确定线与数据点的拟合度，第一步是定义线与每个数据点之间的算术距离。对于数据中的每个X值，线性方程将在线上确定Y值。这个值是预测的Y值，被称为$\hat{Y}$。在这个预测值与数据的实际Y值之间的距离为：

距离值=$Y-\hat{Y}$

误差平方和=$\sum(Y-\hat{Y})^2$

现在我们可以将最佳拟合线定义为误差平方和最小的那条线。相应的方法被称为`最小二乘法`。

用符号表示，线性方程的形式为：$\hat{Y}=bX+a$

$b=\frac{SP}{SS_X}\ (17.2)$，其中SP是积和，$SS_X$是X的平方和。

另一个经常使用的斜率公式是基于X和Y的标准差。这个公式是：$b=r\frac{S_Y}{S_X}\ (17.3)$

其中$S_Y$是Y分数的标准差，$S_X$是X分数的标准差。公式中参数a的值为：$a=M_Y-bM_X\ (17.4)$

`Y的回归方程`是线性方程$\hat{Y}=bX+a\ (17.5)$。其中常数b由公式17.2或17.3确定，常数a由公式17.4确定。这个公式能得出在数据点与直线之间的最小的平方误差。

![](regression1.png)
![](regression2.png)
![](regression3.png)

#### 标准化回归方程
迄今为止，我们讲过了原始数据的回归方程，然而，研究者偶尔也会在找出回归方程之前将X与Y标准化为z分数。这样，得到的公式通常被称为`标准化回归方程`，与原始分数版本相比，它的形式大大被简化了。因为z分数是标准化的。具体来说，一组z分数的平均数总是0，标准差总是1。因此，标准化回归方程变成了：

$\hat{z}_Y=\beta z_X\ (17.6)$

首先注意，我们现在用每个X值的z分数来预测相应的Y值的z分数。另外，注意，斜率常数在原始分数公式中用b表示，现在则用$\beta$表示。因为两组z分数的平均数都是零，因此回归方程中的常数a消失了。最后，当变量X被用来预测变量Y时，$\beta$的值等于X与Y的皮尔逊相关。因此，标准回归方程也可以写成：

$\hat{z}_Y=r z_X\ (17.7)$

因为将原始分数变为z分数的过程往往繁琐，研究者通常用计算原始分数回归方程的版本（公式17.5）来代替标准化形式。

#### 估计的标准误
`估计的标准误`衡量了回归线与实际的数据点之间的标准距离。

为了计算估计的标准误，我们首先需要找出离差的平方和（SS）。离差即实际的Y（从数据中得到的）与预测$\bar{Y}$（从回归线上得到的）之间的距离。其平方和通常被称为SS残差。

SS残差=$\sum(Y-\bar{Y})^2\ (17.8)$

得到的SS值然后除以其自由度，得到了方差：

方差=$\frac{SS}{df}$

估计的标准误的自由度是df=n-2。自由度为n-2，而不是习惯的n-1，理由是我们现在只测量到一条线的离差，而不是到平均数的离差。我们必须知道SP才能找出回归线的斜率（公式中的b值）。而为了计算SP，你必须同时知道X与Y的平均数。找出这两个平均数对数据的变异性加上了两重限制，因此只有n-2的自由度（对SS残差df=n-2的一个更直观的解释是，恰好两个点确定一条直线。如果自由两个数据点，那么，它们总是能完美地与直线拟合，因此将不存在误差。只有当你有多于两个点时，才存在一些决定最佳拟合线的自由）。

计算估计的标准误的最后一步是计算方差的平方根，来得出标准距离。最好的公式是：

估计的标准误=$\sqrt{\frac{SS_{residual}}{df}}=\sqrt{\frac{\sum(Y-\hat{Y})^2}{n-2}}\ (17.9)$

![](regression4.png)

#### 标准误与相关的关系
在第16章中，我们观察到相关的平方提供了对预测的精确性的测量：$r^2$是决定系数，因为它确定了Y的变异性中可以被XY关系解释的那部分的比例。

因为$r^2$测量了Y的变异性中可以被回归方程预测的那部分，我们可以使用$1-r^2$来测量没有预测的部分。因此：

预测变异性=$SS_{regression}=r^2SS_Y\ (17.10)$

不可预测的变异性=$SS_{residual}=(1-r^2)SS_Y\ (17.11)$

注意：当r=1.00时，预测是完美的，没有残差。当相关接近零点时，数据点远离回归线，残差变大。用公式17.11计算$SS_{regression}$，得到估计的标准误为：

估计的标准误=$\sqrt{\frac{SS_{regression}}{df}}=\sqrt{\frac{(1-r^2)SS_Y}{n-2}}$

![](regression5.png)
![](regression6.png)

### 回归方程的显著性检验：回归分析
两个MS值被定义为：
$MS_{regression}=\frac{SS_{regression}}{df_{regression}}(df=1)$

$MS_{residual}=\frac{SS_{residual}}{df_{residual}}(df=1)$

F分数：

$F=\frac{MS_{regression}}{MS_{residual}}(df=1,n-2)\ (17.12)$

SS与自由度的完整分析如图17.6所示。

![](regression7.png)

![](regression8.png)
![](regression9.png)

### 有两个预测变量的多元回归
#### 两个预测变量的回归方程
我们将两个预测变量表示为$X_1$与$X_2$，试图预测的变量被表示为Y。那么，有两个预测变量的多元回归方程一般形式为：

$\hat{Y}=b_1X_1+b_2X_2+a\ (17.13)$

如果三个变量$X_1$、$X_2$与Y，全部被标准化为z分数，那么多元回归方程的标准化形式预测了每个Y值的分数。标准化形式是：

$\hat{z_Y}=\beta_1z_{X_1}+\beta_2z_{X_2}\ (17.14)$

多元回归方程的目的是得到更精确的$\hat{Y}$。像一元回归那样，这个目标由最小二乘法达成。首先，我们定义“误差”为每个个体的预测Y值与实际Y值的差异。然后，将这些误差的平方相加。最后，我们计算那些提供了可能得最小误差平方和的$b_1$、$b_2$与a的值。最后的公式如下：

$b_1=\frac{}{}$



![](http://ou8qjsj0m.bkt.clouddn.com//17-10-26/43744723.jpg)

#### 回归方差所占的百分比与残差
![](http://ou8qjsj0m.bkt.clouddn.com//17-10-26/92716831.jpg)
![](http://ou8qjsj0m.bkt.clouddn.com//17-10-26/24074024.jpg)

#### 根据残差计算$R^2$与$1-R^2$
![](http://ou8qjsj0m.bkt.clouddn.com//17-10-26/9263493.jpg)
![](http://ou8qjsj0m.bkt.clouddn.com//17-10-26/93640934.jpg)

#### 估计的标准误
![](http://ou8qjsj0m.bkt.clouddn.com//17-10-26/27284727.jpg)

#### 多元回归方程的显著性检验：回归分析
![](http://ou8qjsj0m.bkt.clouddn.com//17-10-26/81610865.jpg)
![](http://ou8qjsj0m.bkt.clouddn.com//17-10-26/4215948.jpg)

### 评估每个预测值的贡献
![](http://ou8qjsj0m.bkt.clouddn.com//17-10-26/13325829.jpg)
![](http://ou8qjsj0m.bkt.clouddn.com//17-10-26/73130482.jpg)