# 搜索引擎
![cover](https://img3.doubanio.com/lpic/s6176453.jpg)

[豆瓣链接](https://book.douban.com/subject/4861766/)

    作者: W.Bruce Croft / Donald Metzler / Trevor Strohman
    出版社: 机械工业出版社
    副标题: 信息检索实践
    原作名: Search Engines : Information Retrieval in Practice
    译者: 刘挺 / 秦兵 / 张宇 / 车万翔
    出版年: 2010-6-1
    页数: 309
    定价: 56.00元
    装帧: 平装
    丛书: 计算机科学丛书
    ISBN: 9787111288084

# 2.搜索引擎架构
## 2.1 什么是软件架构
搜索引擎的两个主要目的
- 效果（质量）
- 效率（速度）

## 2.2 基本的构件
- 索引处理(indexing process)
  - 文本采集(text acquisition)
  - 文本转换(text transformation)：将文档转换为索引项(index term)或特征(feature)。
  - 索引创建(index creation)
- 查询处理(query process)
  - 用户交互(user interaction)
  - 排序(ranking)
  - 评价(evaluation)：一个重要的任务是利用日志数据来记录和分析用户的行为。

## 2.3 组件及其功能
### 2.3.1 文本采集
1. 爬虫(crawler)
2. 信息源(feed):RSS
3. 转换
  - 将HTML,XML,PDF,WORD,PPT转换成统一格式和文档的元数据格式。
  - 保证它们使用统一的编码方案进行转换。
4. 文档数据库

### 2.3.2 文本转换
1. 解析器：处理文本词素(token)序列，词素切分。
2. 停用词去除
3. 词干提取器(stemmer)把同一个词干(stem)得到的派生词进行归类。
4. 超链接抽取和分析
5. 信息抽取：抽取句法特征，如名词短语，需要某种形式的句法分析和词性标注(part-of-speech tagging)。抽取具有指定语义内容的特征，例如，命名实体(named entity)识别器，识别如人名、公司名、日期和地点等。
6. 分类器
  - 给文档分配事先定义好的类别标签。
  - 判别一个文档是否是垃圾文档，以及识别文档中的广告。
  - 聚类用于事先没有定义类别标签的基础上，将相关文档聚集在一起。

### 2.3.3 索引的创建
1. 文档统计：简单汇总和记录词、特征及文档的统计信息。结果存在查找表(lookup table)。
  - 索引项在各文档中出现次数（词及更加复杂的特征）
  - 索引项在文档中出现位置
  - 索引项在一组文档（如标记“体育”）中出现次数
  - 按照词素数量统计文档长度
2. 加权：利用文档统计结果计算权值，并将权值存储在查找表中。如if·idf
3. 倒排(inversion):将文本转换组件传递过来的文档-词信息流转换为词项-文档信息，以便于建立倒排索引。
4. 索引分派：将索引分发给多台计算机。

### 2.3.4 用户交互
1. 查询输入
2. 查询转换：用于在生成好序的文档之前和之后改善初始查询。
  - 文本转换技术:在查询文本上，需要进行词素切分、停用词去除和词干提取。
  - 拼写检查(spell checking)和查询建议(query suggestion):向用户提供初始查询的一些候选查询，这些候选查询可能纠正了拼写错误或是对用户所需信息的更规范描述。
  - 查询扩展(query expansion):对查询进行推荐或增加一些额外的词项，在对文档中词项的出现情况分析的基础上进行。
  - 相关反馈(relevance feedback):一种查询扩展技术，利用用户认为相关的文档中出现的词项对查询进行扩展。
3. 结果输出
  - 生成网页摘要(snippets)
  - 强调(highlighting)文档中重要的词和段落
  - 对输出结果聚类以找到文档相关的类别
  - 将相应广告增加到结果中

### 2.3.5 排序
1. 打分机制
  - $\sum_iq_id_i$ ,qi是查询中第i个词项的权值，di是文档词项的权值。
2. 性能优化：降低系统的响应时间，提高查询吞吐量。
  - term-at-a-time
  - document-at-a-time
  - 安全的(safe)的优化方式与不安全的(unsafe)的优化方式
3. 分布式

### 2.3.6 评价
1. 日志
  - 点击日志(点击数据)
  - 驻留时间(dwell time)
2. 排序分析
3. 性能分析

# 3.信息采集与信息源
## 3.1 确定搜索内容
## 3.2 网络信息爬取
1. 抓取网页
2. 网络爬虫
3. 时新性
4. 面向主题的信息采集
5. 深层网络：那些爬虫很难找到的站点（deep Web，也称hidden Web）
  - 私人站点：没有任何指向它的链接
  - 表单结果
  - 脚本页面（JavaScript，Flash）
6. 网站地图(sitemap)
7. 分布式信息采集

## 3.3 文档和电子邮件采集
## 3.4 文档信息源
pull类型信息源常见RSS。

## 3.5 转换问题
## 3.6 存储文档
## 3.7 重复检测
- 完全重复文档检测：checksumming，含有相同字符当顺序不同，具有相同checksum。
- 考虑字符出现位置：cyclic redundancy check，CRC
- 近似重复检测：指纹（fingerprint），生成指纹过程：
  - 首先对文档进行分词。删除不是词的内容（空格，HTML标签等）。
  - 对于给定n，将这些词组合成n-gram。通常是相互重叠的词序列。
  - 其中的一些n-gram被选择用于表示该文档。
  - 对这些被选择的n-gram进行散列，以提高检索效率，并进一步减少文档表示的大小。
  - 将散列值进行存储。

## 3.8 去除噪声
Finn(2001):在网页中主要内容部分的文本会比网页中附加内容的文本含有更少量的HTML标签。

# 4.文本处理
## 4.1 从词到词项
- 搜索引擎之所以有效的原因，是文本的很多含义通过词语出现和共现的次数获得。
- 理解文本的统计性质是理解检索模型和排序算法的基础。

## 4.2 文本统计
- Zipf's Law:第r高频的词的出现次数与r成反比，或者说，一个词在词频统计表中的排名乘以它的词频(f)约等于一个常数(k):
  - $r \cdot f = k$
- Zipf 可表示为：$r \cdot P_r = c$
  - 其中$P_r$表示第r高频词出现的概率
  - c是一常数，对于英语而言，$c \approx 0.1$
- Zipf Law通常对排名靠前和靠后的预测不准确。
- $r \cdot P_r$值的log-log图呈现一条直线。
- Heaps(1978)发现，语料规模与词表大小的关系为：$v = k \cdot n^\beta$
  - v为词汇量大小
  - 语料中共有n个词
  - k和 $\beta$是随着不同语料变化的参数
  - Heaps法则预测当语料规模很小时，新词数量增长非常快。随着语料规模变大，新词数量无限增长，但是增速会变慢。
- 估计数据集和结果集大小
  - 假设词的出现彼此独立
    - $P(a \cap b \cap c) = P(a) \cdot P(b) \cdot P(c)$
    - $P(a) = \frac{f_a}{N}$
    - $P(b) = \frac{f_b}{N}$
    - $P(c) = \frac{f_c}{N}$
    - $f_{abc} = N \cdot \frac{f_a}{N} \cdot \frac{f_b}{N} \cdot \frac{f_c}{N}$
  - 假设词的出现不独立
    - $P(a \cap b \cap c) = P(a \cap b) \cdot P(c | a \cap b)$
  - 估计文档总数
    - $\frac{f_{ab}}{N} = \frac{f_a}{N} \cdot \frac{f_b}{N}$
    - $N = \frac{f_a \cdot f_b}{f_{ab}}$

## 文档解析
- 词素切分：指从文档中的字符序列中获取词的过程。
- 停用词去除
  - 停用词(stopword):第一，这些功能词极其普遍。第二，由于它们的普遍性和功能，这些词很少单独表达文档相关程度的信息。
- 词干提取（stemming）：获得一个词不同变形之间关系的文本处理过程。将一个词由变形（inflection）（如复数、时态）或者派生(derivation)产生的多种不同形式简化为一个共同的例子。
- 短语和n-gram
  - 词性标注器（part-of-speech tagger，POS）：根据上下文信息对文本中的每一个词赋予一个词性标记。一般的词性标记包括NN（单数名词）、NNS（复数名词）、VB（动词）、VBD（动词，过去时）、VBN（动词，过去分词）、IN（介词）、JJ（形容词）、CC（连词）、PRP（代词）和MD（情态动词）。
  - 任何n个词的序列都构成一个短语。这就是所谓n-gram。两个词的序列称为bigram，三个词的序列称为trigram。
  - 所有长度的n-gram构成一个Zipf分布，一些常见短语非常频繁，大量的酸雨只出现一次，n-gram（包括单个词）的“排名-频率”数据比只有词本身更符合Zipf分布。

## 文档结构和标记
- 从HTML tag得到的网页结构信息的有些部分，是排序算法用到的非常重要的特征。
- XML：XQuery

## 链接分析
### 锚文本
- 链接的锚文本集合可以作为链出网页额外的文本属性，可以在排序算法中使用。
- 写锚文本的人一般不是目标网页的作者。这意味着锚文本可能是从另一个角度来描述目标网页。

### PageRank
网页u的PageRank一般公式：

$PR(u) = \frac{\lambda}{N} + (1-\lambda) \cdot \sum_{v \in B_u} \frac{PR(v)}{L_v}$

- $B_u$：指向u的网页集合
- $L_v$：网页v中包含的外链数
- N：需要考虑的网页数
- $\lambda$：进入任何网页的概率，一般取0.15

这个公式也可表达为矩阵等式：R=TR,R是矩阵T的特征向量。

其中R为PageRank向量，T为随机游走模型转移概率矩阵。元素$T_{ij}$表示网页i进入网页j的概率，并且

$T_{ij}=\frac{\lambda}{N}+(1-\lambda)\frac{1}{L_i}$

## 信息抽取
命名实体识别（named entity recognition）：一个命名实体是表达特定应用感兴趣的某一个事物的词或词序列。最一般的例子是人名、公司名或机构名、地名、时间和日期表达式、数量和货币值。

    Fred Smith,who lives at 10 Water Street,Springfield,MA, is a long-time collector of tropical fish.

隐马尔可夫模型的信息抽取：给定一个句子，找到一个实体类别序列，使得产生这个句子的概率最大。自由状态转移时产生的输出是可见的（可以被观察到的）：潜在的状态是隐藏的（hidden）。

对于上面例子，识别器将找到：

    <start><name><not-an-entity><location><not-an-entity><end>

时，对于那个模型，其概率最高。Viterbi算法可以在HMM模型中找到最大概率的状态序列。

使用这种方法识别命名实体的核心问题是，句子模型中的概率必须从训练数据估计出来。为了估计转义和输出概率，使用人工标注了正确实体标签的文本作为训练数据。从这个训练数据，可以直接估计出某个类别产生词的概率（输出概率）和类别之间的转移概率。

## 国际化
字符编码是搜索引擎处理非英语语言时的核心问题。

词素切分对于很多语言很重要，有其是CJK家族。

# 5.基于索引的相关排序
## 抽象的相关排序模型
- 主题（topical）特征
- 质量（quality）特征
  - 链接到该文档的网页数量
  - 这个页面上次更新至今的天数

相关排序函数R：$R(Q,D)=\sum_ig_i(Q)f_i(D)$
- fi是某个特征函数，它从文档文本中获得一个数值。
- gi是一个相似特征函数，它从查询中获得一个值。

## 倒排索引
- 通常认为单词是文档的一部分，但是如果转变这个想法，认为文档是附着在单词上的，这样的索引就被称作倒排（inverted）了。
- 每个索引项有其自身的倒排列表（inverted list），它们含有和该项相关的数据。
- 在搜索引擎中，数据可能是文档列表或者单词所在位置的列表。列表的每个项被称作posting，在posting中，指向特定文档或位置的部分称作指针（pointer）。

## 压缩
- 某处理器每秒处理p个倒排表的posting。
- 该处理器附在每秒能够提供m个posting的存储器上。
- 压缩率为r。
- 处理速度为d的压缩因子。
- 当不使用压缩时，r=1，d=1。任何可行的压缩技术使得r>1,d<1。
- 理想情况是选择一种使得min(mr,dp)最大的压缩方法，这将发生在mr=dp时。
- 有损（lossless）压缩 、无损（lossy）压缩

## 辅助结构
倒排文件通常存储在一个文件中，在倒排文件中，还有一个被称作词汇（vocabulary或lexicon）的目录结构，它包含一个从索引到倒排文件字节偏移量的查找表。

迄今为止介绍的搜索引擎都是返回文档编号的列表和分数。一个真正面向用户的搜索引擎，需要显示关于每个文档的文本信息，如文档标题，URL或者文本摘要。

## 索引构建
### 简单构建
```
procudure BuildIndex(D)       # D是一个文本文档集
  I <- HashTable()            # 倒排表存储
  n <- 0                      # 文档编号
  for all ducuments d in D do
    n <- n + 1
    T <- Parse(d)             # 将文档分拆成标记
    Remove duplicates from T
    for all tokens t in T do
      if It not in I then
          It <- Array()
      end if
      It.append(n)
    end for
  end for
  return I
end procudure
```

两点限制
- 它要求所有的倒排表都存于内存中，对于较大文档集，会存在问题。
- 算法是顺序的，不便于并行。

### 融合
内存问题的典型解法是融合（merging）。可以构建倒排表结构I直到内存耗尽。这时，将部分索引I写到磁盘上，然后开始建造新的索引。最终，磁盘存放了许多部分索引，I1,I2,I3,...,In。然后系统将这些文件融合为一个。

### 更新
索引合并和结果合并。

## 查询处理
- document-at-a-time评价
- term-at-a-time评价
- 优化技术
  - 倒排表跳转
  - 联合处理
  - 阈值方法
  - MaxScore
  - 提早终止
  - 缓存：文档中词频遵从zipf分布

# 6.查询与界面
## 查询转换与提炼
- 停用词去除和词干提取
- 拼写检查和建议
  - 拼写检查工具：拼写字典（spelling dictionary），编辑距离（edit distance）
  - 拼写矫正：噪声通道模型（noisy channel model），一个人在自觉上要输出（即写出）词w，是根据概率分布P(w)，然后这人要去写出词w，但是声音通道（大体上相当于人的大脑）使得这个人以概率P(e|w)写了词e。
    - 语言模型（language model）：P(w)
    - 错误模型（error model）：P(e|w)
    - 一个词的语言模型概率可以采用这个词在文本中出现的概率和它接着前一个词出现的概率混合形式（mixture）来计算：$\lambda P(w)+(1-\lambda)P(w|w_p)$
      - $\lambda$是描述两个概率相对重要程度的参数
      - $P(w|w_p)$ 是词w在词wp之后出现概率
    - 错误模型（error model）：P(e|w)的概率估计
      - 简单方法：假定所有具有相同编辑距离的错误有相等的概率，并且仅考虑在一个确定的编辑距离以内的字符串（通常取编辑距离为1或2）。
      - 复杂方法：对于一些确定类型的错误发生可能性进行概率估计，例如，当想要输入e时输入a，这些概率估计是通过对大规模的文本集中查找一些正确拼写和不正确拼写的词对获得的。
- 查询扩展
  - 衡量词项相关性
    - 戴斯系数（Dice's coefficient）:$\frac{2 \cdot n_{ab}}{n_a+n_b} \triangleq \frac{n_{ab}}{n_a+n_b}$
      - $\triangleq$指公式是排序相等的
    - 互信息（mutual information）用来衡量词之间能够相互独立出现的程度：$log\frac{P(a,b)}{P(a)P(b)}$
      - P(a)给定大小的文本窗口中词a出现的概率
      - P(b)给定大小的文本窗口中词b出现的概率
      - P(a,b)给定大小的文本窗口中词a和b同时出现的概率
    - 期望互信息：根据概率P(a,b)对互信息进行加权处理：
      - $P(a,b) \cdot log\frac{P(a,b)}{P(a)P(b)}$
    - 皮尔森 $\chi^2$检验：计算两个词共现的观测值与这两个词在互相独立条件下共现的期望值的比值，并根据期望值对这个比值进行归一化处理 $\frac{(n_{ab}-N \cdot \frac{n_b}{N} \cdot \frac{n_b}{N})^2}{N \cdot \frac{n_a}{N} \cdot \frac{n_b}{N}} \triangleq \frac{(n_{ab}-\frac{1}{N} \cdot n_a \cdot n_b)^2}{n_a \cdot n_b}$
- 相关反馈:在相关反馈中，不是让用户从词项列表中或可替换的查询中进行选择，而是让用户指出哪些文档是感兴趣的（即相关的），以及哪些是完全离题的（即不相关的）。根据这些信息，系统通过增加词项或对原始词项重新分配权重，自动地改写查询，并用改写的查询生成新的文档排序。
- 上下文和个性化
  - 让用户根据预定义的类别对自己进行描述。
  - 本地搜索（local search），从查询中或者从提交查询的设备的位置信息中获得的地理信息，并根据这些信息调整排序结果。

## 搜索结果显示
- 搜索结果页面与页面摘要：自动文摘技术可以分为查询无关文摘和查询相关文摘。
- 广告与搜索：关键词一般是从网页内容中选取，然后用这些关键词去搜索广告数据库，选择可以与网页内容一起展示的广告。
- 结果聚类：聚类是将检索结果文档按照内容相似聚成一组并标记每个组，使得用户能够很快浏览到相关类别。
  - 逐面分类法（faceted classification）：由一些类别组成，通常这些类别被组织成层次形式，每个类别用一组层面来描述与其相关的一些重要属性。

## 跨语言搜索
- 统计机器翻译模型，平行语料库（parallel corpora）。在平行语料库中的句子采用人工或自动方式对齐。
- 自动音译（transliteration）：用来处理人名问题。

# 7.检索模型
## 检索模型概述


## 概率模型



## 基于排序的语言模型


## 复杂查询与证据整合


## 网络搜索


## 机器学习和信息检索


# 8.搜索引擎评价

# 9.分类与聚类

# 10.社会化搜索
## 用户标签和人工索引
- 搜索标签
  - 挑战
    - 对于复杂项目，标签是非常稀疏的表示方式。查询关键词与标签关键词之间没有重叠（词表不匹配（vocabulary mismatch）问题）。解决办法：1.提取词干；2.通过伪相关反馈丰富稀疏标签
    - 标签内含噪声。由于是由用户创建的，这些标签可能是偏离主题的、不适当的、拼写错误的甚至是垃圾信息。
- 推测缺失的标签
  - 对出现在文本中的每一个词项计算权重，然后选择k个权值最高的词项作为预测的标签。如tfidf。
  - 标签推测问题也视为分类问题。给定标签的固定本体或大众分类，目标是为每一个标签训练出一个二元分类器。
  - 新颖性（novelty）问题：选择既相关又不冗余的标签。
    - 最大边缘相关（maximal marginal relevance，MMR），MMR选择标签时，标签之间不是彼此独立，而是迭代地选择标签，一次为项目添加一个标签。给定一个项目i及相应的标签集合Ti，MMR选择下一个标签时，最大化公式：$MMR(t;T_i)=(\lambda Sim_{item}(t,i)-(1-\lambda)max_{t \in T_i}Sim_{tag}(t_i,t)$
      - $Sim_{item}$ 度量标签t与项目i相似度的方程。
      - $Sim_{tag}$ 度量两个标签间的相似度。
      - $\lambda$平衡相关性($\lambda=1$)和新颖性($\lambda=0$)。
- 浏览和标签云
  - 标签云：描绘标签的流行度或重要性。

## 社区内搜索

# 11.超越词袋
