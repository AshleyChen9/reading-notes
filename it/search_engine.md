# 6.查询与界面
## 查询转换与提炼
### 停用词去除和词干提取
### 拼写检查和建议
- 拼写检查工具：拼写字典（spelling dictionary），编辑距离（edit distance）
- 拼写矫正：噪声通道模型（noisy channel model），一个人在自觉上要输出（即写出）词w，是根据概率分布P(w)，然后这人要去写出词w，但是声音通道（大体上相当于人的大脑）使得这个人以概率P(e|w)写了词e。
  - 语言模型（language model）：P(w)
  - 错误模型（error model）：P(e|w)
  - 一个词的语言模型概率可以采用这个词在文本中出现的概率和它接着前一个词出现的概率混合形式（mixture）来计算：$\lambda P(w)+(1-\lambda)P(w|w_p)$
    - $\lambda$是描述两个概率相对重要程度的参数
    - $P(w|w_p)$ 是词w在词wp之后出现概率
  - 错误模型（error model）：P(e|w)的概率估计
    - 简单方法：假定所有具有相同编辑距离的错误有相等的概率，并且仅考虑在一个确定的编辑距离以内的字符串（通常取编辑距离为1或2）。
    - 复杂方法：对于一些确定类型的错误发生可能性进行概率估计，例如，当想要输入e时输入a，这些概率估计是通过对大规模的文本集中查找一些正确拼写和不正确拼写的词对获得的。

### 查询扩展
- 衡量词项相关性
  - 戴斯系数（Dice's coefficient）:$\frac{2 \cdot n_{ab}}{n_a+n_b} \triangleq \frac{n_{ab}}{n_a+n_b}$
    - $\triangleq$指公式是排序相等的
  - 互信息（mutual information）用来衡量词之间能够相互独立出现的程度：$log\frac{P(a,b)}{P(a)P(b)}$
    - P(a)给定大小的文本窗口中词a出现的概率
    - P(b)给定大小的文本窗口中词b出现的概率
    - P(a,b)给定大小的文本窗口中词a和b同时出现的概率
  - 期望互信息：根据概率P(a,b)对互信息进行加权处理：
    - $P(a,b) \cdot log\frac{P(a,b)}{P(a)P(b)}$
  - 皮尔森 $\chi^2$检验：计算两个词共现的观测值与这两个词在互相独立条件下共现的期望值的比值，并根据期望值对这个比值进行归一化处理 $\frac{(n_{ab}-N \cdot \frac{n_b}{N} \cdot \frac{n_b}{N})^2}{N \cdot \frac{n_a}{N} \cdot \frac{n_b}{N}} \triangleq \frac{(n_{ab}-\frac{1}{N} \cdot n_a \cdot n_b)^2}{n_a \cdot n_b}$

### 相关反馈
在相关反馈中，不是让用户从词项列表中或可替换的查询中进行选择，而是让用户指出哪些文档是感兴趣的（即相关的），以及哪些是完全离题的（即不相关的）。根据这些信息，系统通过增加词项或对原始词项重新分配权重，自动地改写查询，并用改写的查询生成新的文档排序。

### 上下文和个性化
- 让用户根据预定义的类别对自己进行描述。
- 本地搜索（local search），从查询中或者从提交查询的设备的位置信息中获得的地理信息，并根据这些信息调整排序结果。

## 搜索结果显示
- 搜索结果页面与页面摘要：自动文摘技术可以分为查询无关文摘和查询相关文摘。
- 广告与搜索：关键词一般是从网页内容中选取，然后用这些关键词去搜索广告数据库，选择可以与网页内容一起展示的广告。
- 结果聚类：聚类是将检索结果文档按照内容相似聚成一组并标记每个组，使得用户能够很快浏览到相关类别。
  - 逐面分类法（faceted classification）：由一些类别组成，通常这些类别被组织成层次形式，每个类别用一组层面来描述与其相关的一些重要属性。

## 跨语言搜索
- 统计机器翻译模型，平行语料库（parallel corpora）。在平行语料库中的句子采用人工或自动方式对齐。
- 自动音译（transliteration）：用来处理人名问题。

# 7.检索模型
## 检索模型概述
一篇文档如果被判定和一个查询是同一个主题，那么它们是主题相关的。

用户相关性考虑用户在判定相关性时涉及的所有因素。这些因素包括文档的年代、语言、目标受众、新颖性等。

相关性是二元还是多元的：二元相关性就是简单地判定一篇文档是相关的还是非相关的。

### 布尔检索模型
假设在检索到的集合中，所有文档关于相关性都是等价的，同时也假设了相关性是二元的。

### 向量空间模型
一篇文档Di表示为索引词项的一个向量：$D_i=((d_{i1},d_{i2}, ..., d_{it}))$

可以表示为一个词项权值的矩阵：$\begin{matrix}
          & 词项1 & 词项2 & \cdots & 词项t \\
     文档1 & d_{11} & d_{12} & \cdots & d_{1t} \\
     文档2 & d_{21} & d_{22} & \cdots & d_{2t} \\
     \vdots & \vdots & \vdots & \cdots & \vdots \\
     文档n & d_{n1} & d_{n2} & \cdots & d_{nt}
\end{matrix}$

文档可以通过计算表示文档和查询的点之间的距离来进行排序。

余弦相关性：$cos(D_i,Q)=\frac{\sum_{j=1}^t d_{ij} \cdot q_j}{\sqrt{\sum_{j=1}^t d_{ij}^2 \cdot \sum_{j=1}^t q_j^2}}$

tf表示文档中词项的频率，反映了一个词项在文档Di（或查询项）中的重要性。$tf_{ik}=\frac{f_{ik}}{\sum_{j=1}^t f_{ij}}$

idf反映了文档数据集中词项重要性。如果在其中出现过一个词项的文档越多，这个词项在文档之间就越没有区分性，也就对检索越没有用。$idf_k=log\frac{N}{n_k}$

Rocchio(1971)：一种根据用户判定的相关文档来修改查询项的算法，这个算法会使得相关文档向量的平均向量和非相关文档向量的平均向量之间的差异化最大。$q_j'=\alpha \cdot q_j=\beta \cdot \frac{1}{|Rel|}\sum_{D_i in Rel}d_{ij}- \gamma \cdot \frac{1}{|Norel|}\sum_{D_i \in Norel}d_{ij}$
  - $q_j$查询项j的初始权重
  - Rel用户选定的相关文档集合
  - Norel非相关文档集合
  - |.|返回一个集合的大小
  - $d_{ij}$ 是文档i中第j个词项的权值
  - $\alpha,\beta,\gamma$控制每个部分影响的参数，合理的数值分别是8，16，4

## 概率模型
### 将信息检索作为分类问题
当P(R|D)>P(NR|D)时，判定文档D是相关的，其中P(R|D)是相关性的条件概率，P(NR|D)是非相关性的条件概率。

$P(R|D)=\frac{P(D|R)P(R)}{P(D)}$

P（R）是相关性的先验概率（任何文档都是相关的可能性）。将判定一篇文档相关的条件（P(R|D)>P(NR|D)）代入：

$\frac{P(D|R)}{P(D|NR)}>\frac{P(NR)}{P(R)}$

左边部分称为似然比。搜索引擎只需要排序文档，不需要做出分类判断。如果采用似然比作为分值，排序较高的是那些对于属于相关集合具有较高似然值的文档。

假设文档由词项组成，相关文档和不相关文档表示为词项的集合：
- 文档表示为一组二元向量特征，$D=(d_1,d_2,...,d_t)$，其中$d_i$表示词项i出现在文档中，反之为0。
- 朴素贝叶斯假设：词项独立性，这意味着可以 $\prod_{i=1}^t P(d_i|R)$ 来估计P(D|R)，类似可以计算P(D|NR)。

$\frac{P(D|R)}{P(D|NR)}=\prod_{i:d_i=1}\frac{p_i}{s_i} \cdot \prod_{i:d_i=0}\frac{1-p_i}{1-s_i}$
- $p_i$是词项i在相关集合的某篇文档中出现（出现为1）的概率。
- $s_i$是词项i在不相关集合的某篇文档中出现（出现为1）的概率。
- $\prod_{i:d_i=1}$ 文档中值是1的词项概率的连乘。

数学推导：

$\prod_{i:d_i=1}\frac{p_i}{s_i} \cdot (\prod_{i:d=1}\frac{1-s_i}{1-p_i} \cdot \prod_{i:d_i=1}\frac{1-p_i}{1-s_i})\prod_{i:d=0}\frac{1-p_i}{1-s_i}$

$=\prod_{i:d_i=1}\frac{p_i(1-s_i)}{s_i(1-p_i)} \cdot \prod_i\frac{1-p_i}{1-s_i}$

第二项连乘涵盖所有词项，也能覆盖所有文档，所以对于排序能够忽略掉。取对数控制精度问题:

$\sum_{i:d_i=1}log\frac{p_i(1-s_i)}{s_i(1-p_i)}$

假设，在其他信息存在的基础上，查询项中那个没有出现的词项对于相关文档和非相关文档具有相同的出现概率（即pi=si）。也就是说，给定一个文档，文档的分值可以简单地将匹配到的词项概率求和即可。

如果没有相关集合的其他信息，可以额外假设pi是一个常数，si可以被近似估计为整个文档数据集中的词项出现情况（依据是相关文档的数量远小于整体文档集合的大小），假定pi=0.5，给定词项i的权值为

$log\frac{0.5(1-\frac{n_i}{N})}{\frac{n_i}{N}(1-0.5)}=log\frac{N-n_i}{n_i}$

假定：
- N整个文档数据集中所有文档数量
- R和这个查询相关的文档数量
- ni包含词项i的文档数量
- ri包含词项i的相关文档数量

则:
- $p_i=\frac{r_i}{R}$
- $s_i=\frac{n_i-r_i}{N-R}$

在每个数值上加0.5，整体数值上加1:
- $p_i=\frac{r_i+0.5}{R+1}$
- $s_i=\frac{n_i-r_i+0.5}{N-R+1}$

$\sum_{i:d_i=q_i=1}log\frac{(r_i+0.5)/(R-r_i+0.5)}{(n_i-r_i+0.5)/(N-n_i-R+r_i+0.5)}$

### BM25排序算法
通过加入文档权值和查询项权值，拓展了二元独立模型的得分函数：
$\sum_{i \in Q}log\frac{(r_i+0.5)/(R-r_i+0.5)}{(n_i-r_i+0.5)/(N-n_i-R+r_i+0.5)} \cdot \frac{(k_1+1)f_i}{K+f_i} \cdot \frac{(k_2+1)qf_i}{k_2+qf_i}$

- fi词项i在文档中的频率
- qfi词项i在查询中的频率
- k1,k2,K经验设定参数

## 基于排序的语言模型
一元语言模型就是语言中词汇的概率分布。例如，如果文档数据集中只包含5个不同的词语，这个集合一个可能的语言模型是(0.2,0.1,0.35.0.25,0.1)，其中每个数值表示词语出现的概率。如果将每个文档看成是词汇的一个序列，那么语言模型的概率就是预测序列中下一个词语的概率。

n元语言模型使用更长序列来预测词语。一个n元模型预测词语时考察前面的n-1个词语。

将文档表示为语言模型后，同样能够将查询的主题表示为语言模型。这种情况下，语言模型是主题的一种表示。这种主题是搜索信息的人在写下查询时头脑中所想的内容。这就导致了三种基于语言模型的检索概率值：一种是基于从文档语言模型生成查询文本的概率；一种是基于从查询项语言模型生成文档文本的概率；一种是基于对比查询语言模型和文档主题语言模型的结果。

### 查询项似然排序
$P(D|Q) \triangleq P(Q|D)P(D)$

- $\triangleq$ 排序等价
- P(D)文档先验概率
- P(Q|D)给定文档后查询的似然函数

P(D)被假设是始终如一（对所有文档都一样），不影响排序结果。所以检索模型通过P(Q|D)来排序文档，这个概率值采用文档一元语言模型来计算：

$P(Q|D)=\prod_{i=1}^nP(q_i|D)$
- qi是查询项中的词，查询项中有n和词。

估计语言模型概率值：

$P(q_i|D)=\frac{f_{q_i,D}}{|D|}$
- $f_{q_i,D}$ 是词语qi在文档集合D中出现次数。
- |D|表示D中词语的数量。

平滑技术用于避免这种估计问题以及数据稀疏问题，一般的方法是降低（或者打折）文档文本中出现词语的估计概率，并对文本中未出现的词语赋给估计的“剩余”概率。未出现词语的概率通常都是基于整个文档数据集中词语的出现频率来进行估计。如果P(qi|C)是文档集合C的数据集语言模型中词语i的出现概率，那么文档中未出现词语的估计概率为 $\alpha_DP(q_i|C)$ ，其中 $\alpha_D$ 是控制赋予未见词语概率的系数。为了保证概率值和为1，文档中一个出现过的词语的概率估计为:

 $(1-\alpha_D)P(q_i|D)+\alpha_DP(q_i|C)$

Jelinek-Mercer平滑方法估计 $P(q_i|D)$

$P(q_i|D)=(1-\lambda)\frac{f_{q_i,D}}{|D|}+\lambda\frac{c_{q_i}}{|C|}$
- $\alpha_D=\lambda$
- 估计词语qi的概率为 $\frac{C_{qi}}{|C|}$
- $C_{qi}$ 是文档数据集中查询词出现的次数
- |C|是集合中所有词语出现的次数总和

文档得分公式： $logP(Q|D)=\sum_{i=1}^nlog(1-\lambda)\frac{f_{q_i,D}}{|D|}+\lambda\frac{c_{qi}}{|C|}$

狄利克雷平滑方法估计 $P(q_i|D)$

$p(q_i|D)=\frac{f_{q_i,D}+\mu\frac{c_{qi}}{|C|}}{|D|+\mu}$
- $\alpha_D=\frac{\mu}{|D|+\mu}$

文档得分公式： $logP(Q|D)=\sum_{i=1}^nlog\frac{f_{q_i,D}+\mu\frac{c_{qi}}{|C|}}{|D|+\mu}$

## 复杂查询与证据整合


## 网络搜索


## 机器学习和信息检索


# 8.搜索引擎评价

# 9.分类与聚类

# 10.社会化搜索
## 用户标签和人工索引
- 搜索标签
  - 挑战
    - 对于复杂项目，标签是非常稀疏的表示方式。查询关键词与标签关键词之间没有重叠（词表不匹配（vocabulary mismatch）问题）。解决办法：1.提取词干；2.通过伪相关反馈丰富稀疏标签
    - 标签内含噪声。由于是由用户创建的，这些标签可能是偏离主题的、不适当的、拼写错误的甚至是垃圾信息。
- 推测缺失的标签
  - 对出现在文本中的每一个词项计算权重，然后选择k个权值最高的词项作为预测的标签。如tfidf。
  - 标签推测问题也视为分类问题。给定标签的固定本体或大众分类，目标是为每一个标签训练出一个二元分类器。
  - 新颖性（novelty）问题：选择既相关又不冗余的标签。
    - 最大边缘相关（maximal marginal relevance，MMR），MMR选择标签时，标签之间不是彼此独立，而是迭代地选择标签，一次为项目添加一个标签。给定一个项目i及相应的标签集合Ti，MMR选择下一个标签时，最大化公式：$MMR(t;T_i)=(\lambda Sim_{item}(t,i)-(1-\lambda)max_{t \in T_i}Sim_{tag}(t_i,t)$
      - $Sim_{item}$ 度量标签t与项目i相似度的方程。
      - $Sim_{tag}$ 度量两个标签间的相似度。
      - $\lambda$平衡相关性($\lambda=1$)和新颖性($\lambda=0$)。
- 浏览和标签云
  - 标签云：描绘标签的流行度或重要性。

## 社区内搜索

# 11.超越词袋
